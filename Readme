Perfect ‚úÖ ‚Äî you want a **complete `README.md`** for your **RAG HR Assistant** project, including:

* Frontend + Backend setup
* Local Ollama (Llama 3.2 + Nomic-Embed-Text) installation
* Pinecone setup
* Environment variables
* How to run everything together locally

Here‚Äôs a **professional production-ready README.md** you can drop straight into your repo üëá

---

# üß† RAG HR Assistant

An AI-powered HR assistant built using **FastAPI + React + Pinecone + Ollama (Llama 3.2)**.
It allows employees to ask HR-related questions (like leave policy, working hours, etc.) and get accurate answers from stored company policy PDFs using **Retrieval-Augmented Generation (RAG)**.

---

## üìÅ Project Structure

```
rag-hr-assistant/
‚îÇ
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ embeddings_loader.py
‚îÇ   ‚îú‚îÄ‚îÄ .env
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hr_policies/       # PDF files with HR documents
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ .env
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Backend Setup (FastAPI + Pinecone + Ollama)

### 1Ô∏è‚É£ Create a virtual environment

```bash
cd backend
python -m venv venv
venv\Scripts\activate   # (Windows)
# source venv/bin/activate  # (Mac/Linux)
```

### 2Ô∏è‚É£ Install dependencies

```bash
pip install -r requirements.txt
```

Example `requirements.txt`

```txt
fastapi
uvicorn
requests
python-dotenv
pinecone-client
PyMuPDF
```

---

### 3Ô∏è‚É£ Install and Run Ollama locally

Ollama lets you run open-weight LLMs **completely offline**.

#### üß© Install Ollama

Go to üëâ [https://ollama.ai/download](https://ollama.ai/download)

After installing:

```bash
ollama pull llama3.2
ollama pull nomic-embed-text
```

To verify models:

```bash
ollama list
```

You should see:

```
llama3.2      latest
nomic-embed-text latest
```

Keep Ollama running in background ‚Äî it runs on `http://localhost:11434`.

---

### 4Ô∏è‚É£ Setup Pinecone

Create a free account: [https://app.pinecone.io](https://app.pinecone.io)

#### Create an Index

* Name: `hr-policies`
* Dimension: **768**
* Metric: `cosine`
* Cloud: AWS
* Region: `us-east-1`

---

### 5Ô∏è‚É£ Environment Variables

Create a file `.env` inside `backend/`:

```env
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_INDEX_NAME=hr-policies
```

---

### 6Ô∏è‚É£ Load HR Policy PDFs into Pinecone

Place all your HR PDFs inside:

```
backend/data/hr_policies/
```

Then run:

```bash
python embeddings_loader.py
```

‚úÖ This will:

* Read your PDFs
* Generate embeddings via `nomic-embed-text`
* Store them in your Pinecone index

---

### 7Ô∏è‚É£ Start Backend Server

```bash
uvicorn main:app --reload
```

You should see:

```
INFO:     Application startup complete.
```

Backend runs at:
üëâ **[http://localhost:8000](http://localhost:8000)**

Test in browser:
[http://localhost:8000](http://localhost:8000)

---

## üíª Frontend Setup (React)

### 1Ô∏è‚É£ Go to frontend folder

```bash
cd ../frontend
```

### 2Ô∏è‚É£ Install dependencies

```bash
npm install
```

### 3Ô∏è‚É£ Create `.env` in `frontend/`

```env
VITE_API_URL=http://localhost:8000
```

### 4Ô∏è‚É£ Start frontend

```bash
npm run dev
```

It will show something like:

```
Local:   http://localhost:5173/
```

Open that URL in your browser.

---

## üß© Chat Flow

1. Type your question (e.g. *‚ÄúWhat is the annual leave policy?‚Äù*).
2. React frontend calls backend `/ask` API.
3. Backend:

   * Generates an embedding of the question using **nomic-embed-text**.
   * Searches **Pinecone** for the most relevant policy text.
   * Combines top results as context.
   * Sends it to **Llama 3.2** to generate a human-like answer.
4. Response is displayed back to the user.

---

## üöÄ Example Interaction

**User:**

> What are the office working hours?

**Assistant:**

> Office hours are from 9 AM ‚Äì 6 PM, Monday to Friday.

---

## üß™ Test Backend

You can test manually with:

```bash
curl -X POST http://localhost:8000/ask \
     -H "Content-Type: application/json" \
     -d '{"question": "What is the leave policy?"}'
```

Expected JSON response:

```json
{
  "answer": "Employees are entitled to 18 days of annual leave per year."
}
```

---

## üõ†Ô∏è Common Issues

| Issue                                                  | Fix                                                                                   |
| ------------------------------------------------------ | ------------------------------------------------------------------------------------- |
| ‚ùå `Vector dimension 768 does not match the index 1536` | Recreate your Pinecone index with `dimension=768`.                                    |
| ‚ùå `Embedding generation failed`                        | Ensure Ollama is running: `ollama serve`.                                             |
| ‚ùå `CORS Error`                                         | Add `from fastapi.middleware.cors import CORSMiddleware` and allow localhost origins. |
| ‚ùå ‚ÄúSorry, I couldn‚Äôt find anything‚Äù                    | Make sure you ran `embeddings_loader.py` and PDFs exist in `data/hr_policies/`.       |

---

## üß∞ Add CORS Middleware (optional)

If you face CORS errors, add this in `main.py`:

```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

---

## üßæ License

MIT License ¬© 2025 Remarkinfo Software Solutions

---

Would you like me to also generate a **sample `frontend/src/App.jsx` and `backend/requirements.txt`** (clean, error-free, minimal working version) to include in your project?
That way your README + code will run perfectly from scratch on any system.
