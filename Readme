
# ğŸ§  RAG HR Assistant

The **RAG HR Assistant** is an AI-powered HR chatbot built using **FastAPI**, **React**, **Pinecone**, and **Ollama (Llama 3.2)**.
It helps employees quickly find answers from HR policy documents â€” just ask a question, and it responds intelligently using your organizationâ€™s HR data.

---

## ğŸ’¡ What It Does

This chatbot uses **Retrieval-Augmented Generation (RAG)**, which means it:

1. Reads your HR policy PDFs.
2. Stores them as embeddings in **Pinecone**.
3. Uses **Llama 3.2** to generate human-like responses.
4. Runs entirely **locally** with Ollama â€” no external APIs required.

You can ask questions like:

* â€œWhat is our annual leave policy?â€
* â€œWhat are the office working hours?â€
* â€œDo employees get health insurance coverage?â€

---

## ğŸ—‚ï¸ Project Structure

The project is divided into two main parts:

* **Backend (FastAPI)** â€“ Handles document embedding, querying, and LLM response generation.
* **Frontend (React)** â€“ Provides a simple and clean chat interface for users.

The backend connects to **Ollama** and **Pinecone**, while the frontend communicates via REST APIs.

---

## âš™ï¸ Backend Setup

### Step 1 â€“ Create and activate a virtual environment

Inside the `backend` folder, create a Python virtual environment and install dependencies.

### Step 2 â€“ Configure Ollama

Install Ollama from the official website and pull the required models: **Llama 3.2** and **Nomic-Embed-Text**.
Ollama runs locally on port `11434`, so your data never leaves your machine.

### Step 3 â€“ Setup Pinecone

Create a free Pinecone account and a new index named `hr-policies`.
Use dimension `768`, metric `cosine`, and region `us-east-1`.

### Step 4 â€“ Add Environment Variables

In the backend `.env` file, include your Pinecone API key and index name.
This allows the application to connect securely with your Pinecone vector store.

### Step 5 â€“ Load PDF Documents

Add your company HR policy PDFs to the folder `backend/data/hr_policies/`.
Then, run the embedding loader script once to store them into Pinecone.

### Step 6 â€“ Start the Backend

Run the FastAPI server locally.
Once itâ€™s up, your API will be available at `http://localhost:8000`.

---

## ğŸ’» Frontend Setup

### Step 1 â€“ Install dependencies

Navigate to the `frontend` folder and install all required npm packages.

### Step 2 â€“ Add environment variable

In the frontend `.env` file, specify the backend URL (usually `http://localhost:8000`).

### Step 3 â€“ Start the frontend

Run the React app, which will start at `http://localhost:5173`.
Youâ€™ll see a chatbot interface where you can start asking HR-related questions.

---

## ğŸ§© How It Works

1. You type a question in the chat interface.
2. The frontend sends it to the FastAPI backend.
3. The backend:

   * Converts your question into a vector embedding using **Nomic-Embed-Text**.
   * Searches for similar content in **Pinecone**.
   * Combines the most relevant sections as context.
   * Sends the context to **Llama 3.2** for answer generation.
4. The assistant returns a precise, conversational response.

---

## ğŸ§  Example Interaction

**User:** What is the annual leave policy?
**Assistant:** Employees are entitled to 18 days of paid annual leave per year.

---

## ğŸ”§ Common Issues and Fixes

* **Vector dimension mismatch**: Recreate the Pinecone index with dimension `768`.
* **Embedding generation failed**: Make sure Ollama is running in the background.
* **CORS errors**: Allow cross-origin requests in the backend using FastAPI middleware.
* **No answers returned**: Ensure youâ€™ve uploaded HR policy PDFs and run the embedding loader.

---

## ğŸ› ï¸ Technologies Used

* **FastAPI** â€“ Backend framework for APIs
* **React.js** â€“ Frontend for the chat interface
* **Ollama** â€“ Local LLM runtime for embedding and generation
* **Llama 3.2** â€“ The local large language model for answering questions
* **Nomic-Embed-Text** â€“ Embedding model for text vectorization
* **Pinecone** â€“ Vector database for semantic search
* **PyMuPDF** â€“ Extracts text from PDF HR documents

---

## ğŸš€ Running the Full System Locally

1. Start Ollama on your system.
2. Start the FastAPI backend (`uvicorn main:app --reload`).
3. Start the React frontend (`npm run dev`).
4. Open your browser and go to `http://localhost:5173`.
5. Ask any HR-related question and get instant AI-generated responses!

---

## â¤ï¸ Why This Project Matters

This assistant helps organizations:

* Reduce repetitive HR queries.
* Make policy documents easily accessible.
* Empower employees with self-service HR support.

Itâ€™s a simple, private, and efficient way to bring **AI-powered HR automation** into any organization.

---

## ğŸ§© Next Steps

* Add support for multilingual policies.
* Extend context retrieval to multiple departments.
* Deploy on a private server or cloud.
* Connect to live HR systems for dynamic responses.

---

## ğŸ‘ Credits

Built with ğŸ’¡ by leveraging open-source AI tools:

* **Ollama**
* **Llama 3.2**
* **Pinecone**
* **FastAPI**
* **React**

